\documentclass[a4paper]{article}
\usepackage{hyperref}

\usepackage[numbers]{natbib}

\renewcommand{\thesubsubsection}{\arabic{subsubsection}}
\title{CIT433027 ADLR: Project Ideas}
\author{}

\begin{document}
\maketitle

Here, you can find several ideas for projects we collected.
Take this as an inspiration for your project.
Some ideas are rather ”big”, meaning they could result in multiple projects.
After registering your team (including a draft proposal), you will discuss the extent of your final proposal with your assigned tutor.


% Leon
\subsubsection{Investigating Rapid Motor Adaption}
\citet{qi2022inhand} use Rapid Motor Adaption to train a robust in-hand manipulation policy in simulation, which can be deployed on the actual system without any further adaptation.
This is facilitated by a dedicated part of the neural network predicting the current world in an end-to-end learned latent space to make the policy more robust - first conditioned on a set of available states in simulation, later on only those measurable when deployed on the real system.
We want to find the possible performance improvements and the limits of this approach in a simulation-only setting.
% After the initial setup and an analysis of the capabilities, changes to the training, architecture, and parameterization can be investigated with respect to the overall performance.
\begin{itemize}
  \item Define and set up a training environment with your tutor.
  For example, we are interested in generating a robust in-hand manipulation policy.
  Also, we will decide whether you want to use an existing RL framework or implement the algorithm from scratch, depending on your focus.
  \item Analyze the performance gains as well as the limits of the Rapid Motor Adapton algorithm.
  \item Expand the algorithm to tackle these identified problems, for example, a different encoder architecture or changed loss functions, depending on your identified problems and feedback from the tutor.
\end{itemize}

% \subsubsection{Meta Reinforcement Learning for Sim-to-real Domain Adaptation}
% \citet{Arndt19} show that the sim2real problem can be tackled by applying Meta RL to learned low dimensional projections of the action space.
% \begin{itemize}
%  \item Implement the algorithm
%  \item Change the experimental setup to a sim2sim setting because no real robot is available (i.e., try to adapt one simulation to a given one with unknown parameters).
%  \item Investigate how this framework can be applied or extended to classic closed-loop robot control
% \end{itemize}

% \subsubsection{Solving Complex Sparse Reinforcement Learning Tasks}
% When defining Reinforcement Learning Tasks for robots, it is often desirable to stick to sparse rewards in order to avoid reward shaping. Not only does is it ease the setup since a task can be solely defined by its final desired outcome, but the optimal policy can be found "outside of the box" without a human prior given through the reward function. Unfortunately, in big state spaces random exploration is not able to find these sparse success signals. Therefore, \citet{riedmiller2018} introduce SAC-X.
% \begin{itemize}
%   \item Implement the algorithm
%   \item Investigate how the presented method can be used for finer and more dexterous object manipulation e.g. with a hand.
%   \item Another option is, to apply the method to an path planning agent which often has to recover from dead-end-situations in static environments.
% \end{itemize}


% Lennart
\subsubsection{Non-sequential Reinforcement Learning for Hard Exploration Problems}
A way of overcoming the problems associated with sparse rewards in Reinforcement Learning is to utilize ``expert demonstrations'' in promising regions of the state space.
\citet{bauza2024demostart} use (suboptimal) expert demonstrations for providing a curriculum of interesting states to the agent during learning.
In the absence of human experts, \citet{blau2021learning} propose to first generate successful trajectories by RRT-based planning algorithms and then initialize a policy using that data. 

Based on one of the above papers, you could e.g. study one of the following problems (or come up with your own ideas):
\begin{itemize}
  \item Implement different heuristics for choosing states to explore from. Can you also find a metric to decide when to \textit{stop} an episode early?
  \item Implement an entirely different planning algorithms for obtaining expert demonstrations. Analyse the dependency between the quality of the demonstrations and the final performance of the policy.
\end{itemize}


% Dominik
\subsubsection{Tactile Exploration of Objects}
For grasping an object with a robotic hand, often a full 3D model of the given object is required.
Using a (depth-)camera, one can infer the surface of the visible part of the object. However, the surface of the occluded parts is also usually needed, e.g., to robustly grasp the object.
With tactile exploration (i.e., slowly moving the robotic hand until the fingers touch the object), it is possible to observe even the occluded parts of the object.
This tactile information is usually very sparse. The idea is to use a learning-based approach to complete the full shape from this sparse contact information.

\begin{itemize}
  \item Start in 2D: Train a neural network to complete a 2D shape based on a few given points similar to \citet{watkins2019multi} did in the more complex 3D case.
  \item Develop a strategy that determines the best way for the next tactile exploration based on previous points using reinforcement learning.
  \item Bring it into 3D: Complete 3D shapes in the same way as done for 2D
  \item Combine tactile and visual depth information to infer the object's shape
\end{itemize}


\subsubsection{Self-supervised Learning for Robot Grasping}
Although grasping training data can be generated in simulation, it is computationally expensive to find high-quality grasps.
Therefore, the training process should be as data-efficient as possible.
In Deep Learning, self-supervised learning is one prominent way to reduce the required labeled data.
This has been shown to generate useful features across various modalities like images, audio, and text.
In this project, the goal is to apply self-supervised learning to 3D objects and use the resulting features for grasping.
\begin{itemize}
  \item Familiarize yourself with the data2vec framework~\cite{baevski2022data2vec}
  \item Reimplement data2vec for 3D objects using a CNN
  \item Evaluate the learned features in grasping as a downstream task (data will be provided~\cite{winkelbauer2022})
  \item Optional: Replace the CNN with a transformer
  \item Important: This is an advanced project, therefore prior experience with deep learning is recommended
\end{itemize}


\subsubsection{Grasping via Online Adaptation}
One way to find grasps for a given object is to train a grasp success classifier and use that as an objective function for an online grasp optimization.
Specifically, one differentiates through the grasp score prediction network to optimize the grasp parameters (input to the network) such that the grasp score (output of the network) gets maximized.

\begin{itemize}
  \item A dataset of multi-finger grasps will be provided~\cite{winkelbauer2022}
  \item Preprocess the data
  \item Train a grasp score prediction network~\cite{van2020learning}
  \item Implement an optimization procedure to perform grasping inference
  \item Optional: Train a small generative grasping network to provide better starting points for the optimization
\end{itemize}


\subsubsection{Generative Networks for Robot Grasping}
Generative neural networks can be used to directly generate stable grasps for a given unknown object.
For a parallel jaw gripper, the network learns a distribution over 6D end effector poses conditioned on the observed object.
For this application, different architectures are possible: Variational Autoencoder, Generative Adversarial Networks, diffusion-based architectures~\cite{ho2020denoising} and autoregressive architectures~\cite{winkelbauer2022}.
The goal of this project is to apply one of the generative architectures to the problem of grasp generation.
\begin{itemize}
  \item For training, we make use of the existing public training dataset Acronym~\cite{acronym2020}
  \item Preprocess the data
  \item Adapt the generative architecture for robotic grasping
  \item Optional: Extend the approach to generating grasps constraint to lie on a specified part of the object~\cite{lundell2023constrained}
\end{itemize}


% Felix Kroll
\subsubsection{Tactile Material Classification}

\citet{Tulbure2018} (as mentioned in the lecture) proved that one can classify a vast number of materials robustly ($\approx 95 \%$) using only the spatio-temporal signal of a tactile skin.
For this project you can use the data of \citet{Tulbure2018} from \url{https://dlr-alr.github.io/dlr-tactmat/} and choose \emph{one} of the following two options: 

\paragraph{Option 1 - Self-Supervised Learning}
\begin{itemize}
  \item It is way easier to collect unlabeled samples than labeled. Can we learn a representation without the labels? Recent work in this direction is, e.g., a Joint Embedded Predictive Architecture (\citet{assran2023selfsupervised}).
  \item Instead of a Transformer model, you should start with the TacNet (CNN) architecture of \citet{Tulbure2018} and check if you can validate the claims of \citet{assran2023selfsupervised} for the spatio-temporal tactile signals. 
\end{itemize}

\paragraph{Option 2 - Bayesian Deep Learning}
\begin{itemize}
  \item The work of \citet{Tulbure2018} already leverages a mechanism to enable some notion of model uncertainty using Monte Carlo Dropout (\citet{Gal2016}). But how well does this uncertainty enable the model to "know what it does not know"?
  \item Implement the so called "Bayes by Backprop" from the lecture (\citet{Blundell2015}) and compare the two. How well can both models identify unseen data based one the uncertainty? 
  \item Optional: Implement or use other Bayesian Deep Learning methods like ensembles or Laplace approximations for your comparison.
\end{itemize}


% Finn Süberkrüb 16_10_2024
\subsubsection{Active Learning for Physical Models}
In Lectures 4 and 5, you will learn more about “All models are wrong, but models that know when they are wrong are useful.” (inspired by George E. P. Box). In this project, we explore how physical models can be approximated by machine learning and how incorrect behavior can be prevented.
\begin{enumerate}
  \item Simulate a Physical Model: Create a time-invariant nonlinear system with Gaussian noise.
  \item Baseline Model: Generate data and fit a neural network using supervised learning to reproduce the behavior of the Physical Model.
  \item Active Learning: Implement a probabilistic model (e.g., Bayesian Neural Network) and an acquisition function to iteratively query new data points from the Physical Model. Try to minimize the necessary data points while matching the baseline accuracy.
  \item Visualization: Show how the active learning approach reproduces the Physical model with fewer data points than the Baseline Model.
  \item Scaling Complexity: Increase the model’s complexity and demonstrate the benefits of active learning compared to the baseline. It's up to you how pioneering you want to be.
\end{enumerate}
Paper to start with: \citet{Tang2023ActivelyLD}

% Finn Süberkrüb 16_10_2024
\subsubsection{Distance Aware - Input Filtering}
When encountering something for the first time, we approach it with uncertainty, trying to determine if it works as expected. In Lecture 5, you will learn how robots struggle with anomalies, including an example where a human fatality occurred due to such issues. 
\begin{enumerate}
  \item Define, generate, or select a dataset (e.g., acceleration sensor data) containing both in-distribution and out-of-distribution (OOD) data. OOD data could be introduced by abnormal or unexpected sensor readings (e.g., spikes, random noise, or values outside the normal operating range).
  \item Train a deep neural network (DNN) to learn the patterns in your dataset. Apply spectral normalization to the model weights to ensure the hidden layers preserve the distance between in-distribution and OOD data.
  \item Replace the final layer with a Gaussian Process (GP) layer, allowing the model to quantify uncertainty and detect OOD data (anomalies or noise).
  \item Use the uncertainty estimates from the GP layer to design your filter.
  \item Validate the performance of your filter using ground truth data, and compare it to a traditional filter design (e.g., Low-Pass filter).
\end{enumerate}
Paper to start with: \citet{Liu2020}


%\subsubsection{Exploring Munchausen Reinforcement Learning}
%Recently, \citet{vieillard2020munchausen} proposed an appealingly simple, yet surprisingly effective extension to DQN; using the policy for bootstrapping. Exploring the implications of this idea, projects could for example address a (sub) set of the following problems:
%
%\begin{itemize}
%  \item Extend the idea to continuous action spaces (e.g. by augmenting SAC).
%  \item Apply Munchausen RL to robotic tasks and see what it brings to the table. Does it improve the baselines? If yes, under what circumstances? If no, investigate the causes.
%  \item Come up with adaptive strategies to deal with the additional hyperparameters introduced by \citet{vieillard2020munchausen}.
%\end{itemize}


% % Johannes Pitz
% \subsubsection{Unsupervised Skill Discovery / Curiosity}
% Pretraining neural networks in an unsupervised setting is extremely effective for language models. Models such as GPT or Bert can be fine-tuned or used directly on downstream tasks.
% Similarly, RL agents can be pretrained in an environment without an extrinsic reward signal and later be adapted to specific tasks.
% \citet{Laskin2021} compare many different approaches on a unified benchmark (\href{https://github.com/rll-research/url_benchmark}{URLB}).
% \begin{itemize}
%   \item Skim recent literature on unsupervised RL (e.g.~\cite{Hafner2023, Laskin2022, Li2023InternalReward})
%   \item Choose one/come up with your own ideas or modifications.
%   \item Implement and compare them with the results reported by URLB.
% \end{itemize}

% https://neurips.cc/virtual/2024/poster/95213
% https://sites.google.com/view/effective-metric-exploration


%%%%%%%%%%%%%%%%%%%%%
% Johannes Tenhumberg
\subsubsection{Learning the Inverse Problem}
Look at the possibilities for representing inverse problems with neural networks.
~\citet{Ardizzone2018inverse} compare different flavors of GANs, VAEs and INN(theirs) for inverse problems in general.
Extend their work into the robotic context, with more complex kinematics or sensor models.
For the robot kinematic we have objective metrics to describe how well the generation task was performed.
How can we use this knowledge to improve the training?
\begin{itemize}
  \item What is the best approach to represent the high dimensional nullspaces for complex robot kinematics?
  \item When using a simple sensor (contact measurement between two fingers) we have only limited information about the true state of the robot, can we learn the distributions leading to the observed states? 
  \item ~\citet{Lembono2021GAN} use an ensemble of GANs to reduce the mode collapse.
        What other options do we have to improve the generative model? 
        How to measure the performance if the real distribution is not known?

\end{itemize}


% Johannes Tenhumberg
\subsubsection{Coverage Path Planning}
For search and rescue missions (or vacuum cleaners), exploring the environment quickly and efficiently is vital. 
The goal of coverage path planning~\cite{Jonnarth2023lcoverage} is to navigate collision-free in a (possibly unknown) map and visit all unseen regions. 
\begin{itemize}
	\item Build a simple test environment; start with point robot in 2D.
	\item Formulate the objective: how do we measure and represent good coverage?
	\item Add more realism by including a sensor model (e.g., avalanche beacon). 
    \item Add the kinematics to the problem and explore with a robotic arm~\cite{Meli2025RobotExploration}.  
\end{itemize}


% Johannes Tenhumberg
\subsubsection{Self-Supervised Learning for Robot Motion Planning}
In Optimization-based Motion Planning, one formulates the problem of moving collision-free from A to B as 
finding the minimum of an objective function. 
We can use this objective directly to train a neural network without needing expensive data generation. 
This was already demonstrated to solve the Inverse Kinematics of a robot ~\cite{Tenhumberg2023ik}. 
The goal is to extend the idea to the full motion planning problem (a library to compute the objectives is provided).
\begin{itemize}
	\item Investigate the different aspects of motion planning and train individual networks for self-collision, collision with the environment, dynamics, ...  
	\item Combine the various terms into the full motion planning problem.
	\item Replace the default Gradient Descent to update the network weights with a constrained-based optimizer to ensure the feasibility of the predictions.
\end{itemize}


\subsubsection{Learning Robot and Environment Representations}
    A crucial aspect when using learning-based techniques in robotics is the representation and encoding of the relevant aspects of the problem.
    In the context of collision avoidance, the robot itself and the environment with different obstacles need to be modeled. 
The latter is especially relevant when considering dynamic settings with rapid changes.  
The goal is to explore ONE promising representation and analyze its potential for robot motion planning.
\begin{itemize}
	\item Learn convex decomposition of arbitrary maps~\cite{Deng2020cvxnet}.
	\item Predict the Swept Volume of robot motions~\cite{Baxter2020SweptVolume}.
	\item Build a robot-centered signed distance field~\cite{Liu2023linkSDF}.
\end{itemize}


% Johannes Tenhumberg
%\subsubsection{Learning to Optimize Motion Planning}
%Explore the ideas proposed by "Learning to Optimize" ~\citet{LiM16b} in the context of optimization based motion planning \citet{Zucker2013}.
%Can Reinforcement Learning guide an optimizer to speed up robotic path planning?
%How does it relate to the approach of "Unsupervised Path Regression Networks" ~\citet{Pandy2020}.
%Can we combine those ideas?
%
%\begin{itemize}
%  \item Set up an optimizer for a simple robot (with help from the tutor).
%  \item Test ideas to guide the optimization problem of motion planning.
%  \item What are advantages of this hybrid approach over using RL directly on motion planning?
%\end{itemize}

% Johannes Tenhumberg
%\subsubsection{Motion Planning with Diffusion}
%Look into generative models for robotic motion planning based on the work by \citet{Janner2022diffusionPlanning}.
%The core of their approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories.
%\begin{itemize}
%  \item Use a simple 2D robot to get familiar with the diffusion approach in the robotic context.
%  \item Include the changing environment as a central part of the planning.
%  \item Extend the framework to more complex robots.
%  \item Alternatively, the Inverse Kinematics, with its inherent ambiguity, is a second promising testbed for the diffusion models. 
%\end{itemize}

% Johannes Tenhumberg
%\subsubsection{Harnessing Reinforcement Learning for Neural Motion Planning}
% \citet{Jurgenson2019} tackle motion planning with RL.
% Random exploration does not always find a feasible solution for difficult cases.
% By using RRT* to generate expert knowledge they can guide the exploration more efficiently.
% A comparison between pure DDPG, DDPG+HER, and DDPG-MP(theirs) shows the potential of this approach.
% \begin{itemize}
%   \item Modify their code  for a planar robotic arm it for different robots and environments.
%   \item Is this expert knowledge necessary or can this also be achieved with a well designed curriculum?
%   \item Look at modern approaches to represent the environment in which the robot moves (ie. Basis Points Set ~\citet{Prokudin2019}; PointNet for Motion Planning ~\citet{Strudel2020})
% \end{itemize}


%
\subsubsection{Trajectory Planning with Moving Obstacles}
Drones not only have to plan flight paths through static environments, but also avoid collisions with dynamic objects.
To learn such trajectories, a suitable encoding of the changing environment is crucial.
Start with the Basis Points Set ~\cite{Prokudin2019BPS} and extend it to dynamic environments.
Use this representation for neural motion planning ~\cite{Qureshi2021MPN}.
\begin{itemize}
  \item Come up with a state representation for dynamic environments.
  \item Set up a simple 2D (and later 3D) environment in which an agent can navigate through moving obstacles.
  \item Use RL to plan optimal trajectories in this environment.
  \item Optional: Extend the method to work with uncertainties in the motion prediction of the collision objects.
\end{itemize}


%\subsubsection{Learning to Fly Beyond RL – Analytic Policy Gradient}
%Fixed-wing VTOL (Vertical Take-Off and Landing) drones combine the ability to take off vertically and exploit lift for efficient cross-country flight. The disadvantage is more complicated control. Learned controllers are a promising solution here. \citet{wiedemann2023training} presents a method to directly exploit the differentiability of models and thus skip the detour via reinforcement learning.
%\begin{itemize}
%  \item Setting up the learning pipeline with a simple quadrotor model (simple control).
%  \item Replace the policy with a PID controller. Exploit the pipeline for automatic parameter tuning.
%\end{itemize}


%\subsubsection{Learning to Fly}
%Fixed-wing VTOL (Vertical Take-Off and Landing) drones are highly efficient in long-range flight, but difficult to control during gusty landing phases.
%\citet{ModelAgnosticVTOL} presented an deep learning based model-agnostic VTOL controller. With a similar goal \citet{LearningToFly} introduced an error convolution input enabling the learned controller to adapt for different airframes.
%\begin{itemize}
%  \item Transfer one of the approaches to VTOL drones with only two propellers and control surfaces.
%  \item Expand the learning to continuous action spaces.
%  \item Investigate what sensor readings could be added to the state space to increase stability in gusty conditions.
%  \item Utilize our provided drone model implemented in Julia~\cite{WebJulia} as the high-efficient, flexible and dynamic programming language of the future.
%\end{itemize}
%You will be provided with two Julia examples demonstrating RL in the environment, allowing a instant start even without prior Julia experience.
%With promising results during the midterm presentation, there is the possibility to try the controller with a real VTOL. %However, due to high model uncertainties regarding the control surfaces, the sim-to-real success is doubtful.


%\subsubsection{Flying in true aerodynamics}
%Learned flight controllers which directly output motor signals are often unstable due to insufficient models. Most approaches therefore continue using PID controllers as their last authority. With improved models, learned controllers could significantly outperform traditional PID controllers, especially at the motor level, as complicated aerodynamic effects can be learned and utilised.
%\citet{CrazyflieRL} presented a learned controller that can stabilise a quadcopter for an average of 4s (max. 18s).
%\begin{itemize}
%  \item Sim-to-real transfer, explore different methods of bridging the gap between the simulated model and reality.
%  \item Learning model parameters from real flight data.
%  \item Utilize our provided drone model implemented in Julia~\cite{WebJulia} as the high-efficient, flexible and dynamic programming language of the future.
%\end{itemize}
%You will receive an example code that works in simulation, so that you can get started immediately even without previous Julia experience. In addition, you will receive the necessary hardware (quadcopter), positioning systems and radio links.
%Attention:
%\begin{itemize}
%  \item Real hardware causes more problems than simulators.
%  \item We can only provide the hardware. You must have a space to fly and to set up the lighthouse position system \citet{Lighthouse}, at least 2x2x2m.
%  \item It is advantageous if your team can meet in person at your flying arena.
%\end{itemize}

% Lennart
\subsubsection{Recurrent Off-Policy Reinforcement Learning in POMDPs}
In partially observable Markov decision processes (POMDPs), an RL agent has to be equipped with some sort of memory in order to be able to act optimally.
A well-known method addressing this issue is to encode the history of observations by recurrent neural networks (RNNs).
For example, for the class of off-policy methods, \citet{heess2015} combine RNNs with the DDPG algorithm, and \citet{kapturowski2018} study the interplay of DQN-based algorithms with recurrent experience replay.
Based on this work:
\begin{itemize}
  \item Your tutor will provide you with an environment that requires the use of memory to be solved optimally.
  \item Implement a recurrent version of the SAC algorithm by~\citet{Haarnoja2018a}.
  \item Assess the effect of different design choices and hyperparameters (e.g.~hidden state initialization strategy in the experience replay buffer, truncated BPTT, ...)
\end{itemize}

\subsubsection{Differentiable Bayesian Filters}
\textit{*Prior knowledge of Bayesian filters highly recommended*}\\
Differentiable filters are a promising approach for combining the algorithmic structure of Bayesian filter techniques with the power of learning-based methods (for an overview of existing methods, see, e.g., \citet{kloss2021train}).
Importantly, differentiable filters offer a systematic way of dealing with aleatoric uncertainty in state estimation problems.
In this project you will:
\begin{itemize}
  \item Implement a \textit{differentiable} filter of your choice, for example, EKF, UKF, or Particle Filter.
  \item Your tutor will provide you with a dataset of a pose estimation problem for supervised learning and the code to modify the data generation.
  \item Train the filter on the dataset and conduct one or more of the follwing experiments:
  \begin{itemize}
    \item Compare the performance of your differentiable filter with standard filter variants (where available) on the dataset.
    \item Extend the filtering problem to estimation of physical parameters of a system, like mass or friction coefficients.
    \item Compare the bayesian filter to baselines using Recurrent Neural Networks or Transformers. How do they compare in the low-data and high-data regimes?
  \end{itemize}
\end{itemize}


% Felix Kroll
% \subsubsection{Representations for Tactile Exploration}

% As described above it is often the case that your 3D information about the manipulated object is incomplete or super sparse.
% In order to collect more data the robot hand must explore the object.
% This can be done by utilizing (reinforcement) learning methods.
% However, the policy needs to have a memory of known as well as unknown area in order to find the next exploring action.
% Based on what representation can we encode this information best?

% \begin{itemize}
%   \item Implement a simple 2D environment with a static object (box or multiple boxes 'glued' together) and a moveable probe (like a turtle bot)
%   \item Use a reinforcement learning algorithm of your choice (PPO, SAC etc.) to learn a strategy that can explore the shape as good as possible by using the following representations:
%   \begin{itemize}
%     \item Point clouds with a PointNet++ architecture (\citet{NIPS2017_d8bf84be})
%     \item Sparse occupancy octree encoding with everything marked as occupied at first
%     \item Basis points from ~\citet{Prokudin2019}
%     \item Something else ...
%   \end{itemize}
%   \item In case there is time left you can explore the 3D case or think about the dynamic case when the explored object is not fixed...
% \end{itemize}




% % Johannes Pitz
% \subsubsection{Comparing Methods for Uncertainty Estimation}
% Interesting methods include MC-Dropoout \citet{Gal2016}, Bootstrapping \citet{Osband2018}, and Normalizing Flows \citet{louizos2017multiplicative}. These methods could be compared in vastly different settings.
% \begin{itemize}
%   \item Investigate how the uncertainty estimation changes during the training process (relevant to RL since we generally don’t update the networks until convergence before collecting more data).
%   \item Investigate which methods are best suited for active learning in the framework proposed by \citet{gal2017active}.
%   \item Investigate which methods perform best for DQNs in simple environments similar (\citet{BSuit2020}, \citet{touati2018randomized}).
%   \item Come up with your own ideas.
% \end{itemize}

% % Johannes Pitz
% \subsubsection{Offline Datasets for Reinforcement Learning}
% Offline/Batch RL (learning without interacting with the environment) has recently gained more attention, e.g. \citet{nair2020}, \citet{AWOpt2021}. \\
% Available datasets: \url{github.com/rail-berkeley/d4rl}, \url{github.com/deepmind/deepmind-research/tree/master/rl_unplugged}.
% \begin{itemize}
%   \item Compare different Batch RL algorithms.
%   \item Test new environments.
%   \item Benchmark against online algorithms.
% \end{itemize}
% % \citet{Agarwal2019},

% Johannes Pitz
\subsubsection{Geometric Representations in Reinforcement Learning}
Note: Requires previous experience with GNNs \citet{kipf2016semisupervised}, ideally with pytorch-geometric (\href{https://github.com/pyg-team/pytorch_geometric}{link}).
\begin{itemize}
  \item Similar to \citet{Wang2018nervenet}. Modify PyBullet environments (Hopper, Walker, HalfCheetah, Ant) such that the observations contain a graph representing the robot.
  \item Use message passing network(s) in addition or instead of the MLP for value/Q function and policy in standard algorithms like PPO \citet{Schulman2017} or SAC \citet{Haarnoja2018a}.
\end{itemize}

% % Johannes Pitz
% \subsubsection{Decision Transformer}
% The transformer architecture is extremely effective for language models, and has shown promising results on computer vision tasks.
% Therefore, researches are exploring the application of those models for reinforcement learning, e.g., \citet{DecisionTransformer2021}, \citet{Kuang-Huei2022}. Note that in this research, sequential decision problems are not solved via reinforcement learning, instead the transformer is trained on entire trajectories.
% In your project you could:
% \begin{itemize}
%   \item Train a decision transformer for a (simple) benchmark task.
%   \item Analyze the evaluation performance in detail, improve through different planning approaches.
%   \item Adjust intermediate training signals to improve performance.
% \end{itemize}

% Johannes Pitz
\subsubsection{Diffusion Policies}
Diffusion-based architectures have shown exciting results in image generation.
Recently, researchers started to apply them to policy learning. 
% ~\cite{ho2020denoising}
\citet{Chi2023DiffusionPolicy} show promising imitation learning results with both vision- and state-based policies.
\begin{itemize}
  \item Apply the diffusion policy~\cite{Chi2023DiffusionPolicy} to simple benchmark environments with expert offline data (\href{https://github.com/Farama-Foundation/Minari}{link}).
  % (\href{https://github.com/Farama-Foundation/d4rl/wiki/Tasks}{\url{d4rl/wiki/Tasks}}).
  \item Analyze if the approach is better suited for specific types of environments (e.g., planning vs. control).
  \item Adapt the diffusion architecture for other types of environments.
  \item Optional: Develop and implement ideas on how the architecture can be used in a reinforcement learning setting. 
  % Exploration
\end{itemize}

% % Leon
% \subsubsection{Investigate the $RL^2$ Meta-Reinforcement Learnging Algorithm}
% The $RL^2$ Meta-Reinforcement Learning algorithm was evaluated by \citet{Yu2017} showing the potential performance.


% Leon
\subsubsection{Casting Sim2Real as Meta-Reinforcement Learning}
The $RL^2$ algorithm evaluated by \citet{Yu2017} promise sample-efficient meta-reinforcement learning, meaning that the algorithm can quickly adapt to new unseen tasks. We would like use the capabilities to handle heavily randomized environments occurring in simulations that are designed to allow a real-world transfer of the policy.
\begin{itemize}
  \item Find a suitable benchmark environment.
  \item Implement the algorithm on top of an existing basic RL algorithm and compare the performance.
  % \item Extend it to allow a continuous task distribution during training.
  \item Examine the reward for constant as well as dynamically changing environments.
  \item Analyze different loss terms and evaluate the performance.
  \item Extend the algorithm to deal with problems occurring in Sim2Sim settings with unknown disturbances.
\end{itemize}



% Johannes Pitz
\subsubsection{Information Bottleneck / Ignoring Noise}
The reinforcement learning framework allows us to specify arbitrary observation spaces.
For robotic tasks, in particular, we often intuitively understand what observations might be necessary to solve a given task.
However, it is usually unclear what additional (readily available) information benefits training times and real-world performance.
For practical purposes, it would be convenient to pass all the available information to the agent.
That raises several questions:
\begin{itemize}
  \item Can we pass too much information?
  \item Is redundant information harmful or maybe beneficial (similar to over-parameterization)?
  %  - x, e, x+e
  \item Can the agent learn to ignore noise inputs?
\end{itemize}
Explore those questions on simple environments (\href{https://gymnasium.farama.org/environments/mujoco/}{link}) and come up with network architectures, such as self-attention (cf. \citet{Tang2020}) or simple world models (encoder/decoder nets), to fix arising problems.
% (For example, \citet{Tang2020} show that for image inputs imposing self-attention bottlenecks increases parameter efficiency and generalization performance.
% % Johannes Pitz
% \subsubsection{Taking away privilied information later during training}
%  - auto encoder decrease dimension
%  - increase noise


% % Johannes Pitz
% \subsubsection{Stability of Policies under added Noise}
% When training an robotic agent in simulation and later deploying it in the real world there will always be sim2real gap.
% In this project you could investigate if some network architectures are less prone to fail due to such a gap than others.
% Your focus could also be what types of noise need to be applied during training to obtain robust policies (cf. \citet{Sinha2021}).
% \begin{itemize}
%   \item Train agents (PPO, SAC) with different network architectures (MLP, RNN) on \href{https://gymnasium.farama.org/environments/mujoco/}{simple environments}.
%   \item Come up with noise models (gauss, OU, sticky actions...) that can be applied during training or evaluation.
%   \item Evaluate whether some network architectures are more robust than others.
% \end{itemize}
% https://openreview.net/pdf?id=z2zmSDKONK
% https://arxiv.org/pdf/2101.08452.pdf

% % Johannes Pitz
% \subsubsection{Procgen Benchmark}
% Implement and experiment with different agents on this procedurally-generated benchmark \citet{procgen2020}.
% You can compare your results with other submissions on the leaderboard (\url{github.com/openai/procgen}).


\subsubsection{Model Predictive Control for Robotic Manipulation}

Model predictive control (MPC) is a popular control strategy in robotics. 
Prior implementations in simulation~\cite{MujocoMPC} often assume that the underlying physical model is precisely known, 
which is, however, rarely the case in practice. 
Instead, Domain Randomization (DR) should be applied to cover the range of possible physical behaviors encountered in the real world.
This project aims to investigate the limitations of simple MPC-based approaches in the presence of model uncertainty.

\begin{itemize}
  \item If you feel comfortable coding in C++, extend the implementation provided in~\cite{MujocoMPC} to include randomization of different physical parameters (friction, masses, contact behavior ...).
  \item If not, implement a simple stochastic MPC~\cite{Howell2022-uj} in Python and a simple test environment that allows for randomization of physical parameters (with the help of your tutor).
  \item Analyze the sensitivity of the MPC controller to the randomization of different types of physical parameters. 
\end{itemize} 

% Leon
\subsubsection{Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience}
In~\citet{Chebotar2018}, an efficient method for solving the sim2real problem by iteratively adapting the simulation parameters to the real system is proposed.
% Since the physics simulations like pybullet, MuJuCo or Isaac Gym provide no gradient they need to use relative entropy policy search by~\citet{peters2010}.
% However you could also learn to predict the simulation parameter distribution based on a given trajectory which are generated by the learned policy in simulation.
\begin{itemize}
  \item Create an experimental sim2sim setup because no real robot is available (i.e., try to adapt one simulation to a given one with unknown parameters).
  \item Re-implement their algorithm based on the relative entropy policy search or using reinforcement learning algorithms.
  \item Investigate the convergence of estimated parameters.
  \item Investigate the influence of a broader starting distribution on the final performance.
  \item Investigate how re-using learned policies from previous iterations for initialization of the new network shortens training time and restrains the final performance.
\end{itemize}


% Competition
\subsubsection{Residual Policies for Factory Manipulation}

For this project, we will provide a MuJoCo environment with two robot arms and a conveyor belt:~\url{https://youtu.be/IpwQPmgOLW0?si=mNzBQyJiv-8xCxzo/}.
The task is to pick as many objects as possible from the conveyor belt and put them into a basket. 
We also provide you with a baseline policy that solves the task using classic methods from robotic control.
Exploring the idea of \textit{Residual Policy Learning}~\cite{Silver2018-kz}, your goal in this project is to improve upon the baseline by learning a residual policy by RL.

% Ulf
\subsubsection{Representing Shapes as Latent Codes}
Efficient representations of 3D geometries are crucial for many robotic related task. 
For example, in-hand manipulation or grasping can be conditioned on the geometry of the object. 
Another task is tactile shape detection, where the geometry of an unknown object should be determined. 
\citet{park2019deepsdf} use a "auto-decoder" structure to find a latent parametrization of 3D geometries.
\begin{itemize}
  \item Start off with a simple 2D data set (e.g. segmentation masks), then transfer to 3D (e.g. single category of Shapenet)
  \item Train an auto-decoder architecture as in \cite{park2019deepsdf} to find a latent space representation of shapes.
  \item Implement and investigate improvements on the original paper:
  \begin{itemize}
    \item Add additional regularization on the normal vectors as suggested in \cite{gropp2020implicit}.
    Investigate whether these regularization lead to a more complete and continuous latent space.
    \item Add the mode switching suggested in \cite{liu2022regularized}. 
    Investigate whether this can also improve the shape reconstruction or just leads to better behavior far away from the object.
    \item Investigate how the position encoding, suggested in \cite{comi2024touchsdf}, helps with high frequency features.
    \item Think of ways to use the learned latent space for generative models. Maybe one can make the auto-decoder variational?
  \end{itemize}
\end{itemize}

% Ulf
\subsubsection{Learning the Model of a Tactile Skin}
Tactile sensing is crucial for fine manipulation with robotic hands. 
Modeling a tactile skin sensor, however, requires soft contacts to capture the softness of the fingers. 
This is computationally expensive and, hence, inefficient when used together with reinforcement learning. 
\citet{narang2021sim} train a neural network to map basic contact information such as the contact point and force to the deformation and the sensor response.
Our sensor model \cite{kasolowsky2024fine} includes an expensive raycast operation and an equilibrium search that slow down the simulation. 
This could be mitigated by learning the sensor model beforehand.
\begin{itemize}
  \item Familiarize yourself with our sensor model (code will be provided).
  \item Think of an efficient representation of the local geometry.
  For instance, in \cite{zobeidi2021deep} they learn a directional distance function that could replace the raycasting.
  \item Learn the mapping for the correct deformation. This can either be done in an end-to-end fashion or starting from the raycast that provides the local geometry.
\end{itemize}
%\subsubsection{Policy Adaptation during Deployment}
%Training RL agents in simulation can accelerate learning substantially, and may sometimes be the only viable option due to hardware constraints. However, it is difficult to model the system in simulation accurately enough such that trained policies also work in the real world. \citet{Hansen2021} propose to close this sim2real gap using self-supervision during deployment.
%\begin{itemize}
%  \item Validate the proposed algorithm in a sim2sim setting.
%  \item If everything works correctly we can also run real world experiments with Justin's DLR Hand II.
%\end{itemize}

% \subsubsection{NeRF based Kinematic Calibration}
% Neuronal radiance fields are quite popular these days in the computer vision community.
% While they are often used for novel view synthesis, to create impossible camera effects with real world footage, they also provide a new way to calibrate camera extrinsics and intrinsics \citet{lin2021barf, Sucar:etal:ICCV2021, wang2021nerfmm, SCNeRF2021}.
% This is possible because the extrinsics and intrinsics can be trained together with the model.

% We still require tracking of visual markers to calibrate the kinematic of a robot (see the work of \citet{Birbach2014} as an example).
% However, implicit neuronal scene representations might enable us to learn the robot's kinematic end-to-end.

% \begin{itemize}
%   \item Try to utilize a sim-setup (pybullet) with a simple synthetic pinhole camera and a robot arm that holds the camera. (Perfect rendering is not required!)
%   \item Use the work of \citet{TensoRF} as a fast converging baseline implementation of NeRF and implement your own differentiable camera and kinematic model.
%   The work of \citet{SCNeRF2021} should provide a starting point how the differentiable camera model can be implemented successfully.
%   \item Investigate the pros and cons of such a differentiable calibration using NeRFs.
%   \item How can we integrate the notion of uncertainty into the gradient descent based optimization of the kinematic model?
% \end{itemize}



% \subsubsection{Compare Bullet/MuJoCo/Isaac by implementing DLR Hand II Environment}
% Need to have GPU.




%\subsubsection{Generalization in Environments with Continuous Action Spaces}
%\citet{igl2019} highlighted improvements to boost generalization performance of RL algorithms in maze-like environments. Investigate how these tweaks can be applied to more physics-inspired tasks including domain randomization and continuous action spaces.




\bibliographystyle{unsrtnat}
\bibliography{minimal-research}

\end{document}
